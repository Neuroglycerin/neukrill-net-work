{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes setting up a model inspired by the Maxout Network (Goodfellow et al.) which they ran out the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yaml file was modified as little as possible, substituting variables for settings and dataset paths, getting rid of their data pre-processing, and changing the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!obj:pylearn2.train.Train {\n",
    "    dataset: &train !obj:neukrill_net.dense_dataset.DensePNGDataset {\n",
    "            settings_path: %(settings_path)s,\n",
    "            run_settings: %(run_settings_path)s,\n",
    "            training_set_mode: \"train\"\n",
    "    },\n",
    "    model: !obj:pylearn2.models.mlp.MLP {\n",
    "        batch_size: &batch_size 128,\n",
    "        layers: [\n",
    "                 !obj:pylearn2.models.maxout.MaxoutConvC01B {\n",
    "                     layer_name: 'h0',\n",
    "                     pad: 4,\n",
    "                     tied_b: 1,\n",
    "                     W_lr_scale: .05,\n",
    "                     b_lr_scale: .05,\n",
    "                     num_channels: 96,\n",
    "                     num_pieces: 2,\n",
    "                     kernel_shape: [8, 8],\n",
    "                     pool_shape: [4, 4],\n",
    "                     pool_stride: [2, 2],\n",
    "                     irange: .005,\n",
    "                     max_kernel_norm: .9,\n",
    "                     partial_sum: 33,\n",
    "                 },\n",
    "                 !obj:pylearn2.models.maxout.MaxoutConvC01B {\n",
    "                     layer_name: 'h1',\n",
    "                     pad: 3,\n",
    "                     tied_b: 1,\n",
    "                     W_lr_scale: .05,\n",
    "                     b_lr_scale: .05,\n",
    "                     num_channels: 192,\n",
    "                     num_pieces: 2,\n",
    "                     kernel_shape: [8, 8],\n",
    "                     pool_shape: [4, 4],\n",
    "                     pool_stride: [2, 2],\n",
    "                     irange: .005,\n",
    "                     max_kernel_norm: 1.9365,\n",
    "                     partial_sum: 15,\n",
    "                 },\n",
    "                 !obj:pylearn2.models.maxout.MaxoutConvC01B {\n",
    "                     pad: 3,\n",
    "                     layer_name: 'h2',\n",
    "                     tied_b: 1,\n",
    "                     W_lr_scale: .05,\n",
    "                     b_lr_scale: .05,\n",
    "                     num_channels: 192,\n",
    "                     num_pieces: 2,\n",
    "                     kernel_shape: [5, 5],\n",
    "                     pool_shape: [2, 2],\n",
    "                     pool_stride: [2, 2],\n",
    "                     irange: .005,\n",
    "                     max_kernel_norm: 1.9365,\n",
    "                 },\n",
    "                 !obj:pylearn2.models.maxout.Maxout {\n",
    "                    layer_name: 'h3',\n",
    "                    irange: .005,\n",
    "                    num_units: 500,\n",
    "                    num_pieces: 5,\n",
    "                    max_col_norm: 1.9\n",
    "                 },\n",
    "                 !obj:pylearn2.models.mlp.Softmax {\n",
    "                     max_col_norm: 1.9365,\n",
    "                     layer_name: 'y',\n",
    "                     n_classes: %(n_classes)i,\n",
    "                     irange: .005\n",
    "                 }\n",
    "                ],\n",
    "        input_space: !obj:pylearn2.space.Conv2DSpace {\n",
    "            shape: &window_shape [32, 32],\n",
    "            num_channels: 3,\n",
    "            axes: ['c', 0, 1, 'b'],\n",
    "        },\n",
    "    },\n",
    "    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
    "        learning_rate: .17,\n",
    "        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {\n",
    "            init_momentum: .5\n",
    "            },\n",
    "        train_iteration_mode: 'even_shuffled_sequential',\n",
    "        monitor_iteration_mode: 'even_sequential',\n",
    "        monitoring_dataset:\n",
    "            {\n",
    "                'test' : !obj:neukrill_net.dense_dataset.DensePNGDataset  {\n",
    "                                settings_path: %(settings_path)s,\n",
    "                                run_settings: %(run_settings_path)s,\n",
    "                                training_set_mode: \"test\"\n",
    "                },\n",
    "            },\n",
    "        cost: !obj:pylearn2.costs.mlp.dropout.Dropout {\n",
    "            input_include_probs: { 'h0' : .8 },\n",
    "            input_scales: { 'h0' : 1. }\n",
    "        },\n",
    "        termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {\n",
    "            max_epochs: 474 \n",
    "        },\n",
    "    },\n",
    "    extensions: [\n",
    "        !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {\n",
    "            start: 1,\n",
    "            saturate: 250,\n",
    "            final_momentum: .65\n",
    "        },\n",
    "        !obj:pylearn2.training_algorithms.sgd.LinearDecayOverEpoch {\n",
    "            start: 1,\n",
    "            saturate: 500,\n",
    "            decay_factor: .01\n",
    "        },\n",
    "        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {\n",
    "            channel_name: test_y_misclass,\n",
    "            save_path: '%(save_path)s'\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start training the model, we need to create a dictionary with all preprocessing settings and poimt to the yaml file corresponding to the model: `cifar10.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_settings = {\n",
    "    \"model type\":\"pylearn2\",\n",
    "    \"yaml file\": \"cifar10.yaml\",\n",
    "    \"preprocessing\":{\"resize\":[48,48]},\n",
    "    \"final_shape\":[48,48],\n",
    "    \"augmentation_factor\":1,\n",
    "    \"train_split\": 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the path for settings, utils and os must be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'neukrill_net.utils' from '/afs/inf.ed.ac.uk/user/s13/s1320903/Neuroglycerin/neukrill-net-tools/neukrill_net/utils.pyc'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neukrill_net.utils\n",
    "import os\n",
    "reload(neukrill_net.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_settings[\"run_settings_path\"] = os.path.abspath(\"run_settings/cifar10_based.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'augmentation_factor': 1,\n",
       " 'final_shape': [48, 48],\n",
       " 'model type': 'pylearn2',\n",
       " 'preprocessing': {'resize': [48, 48]},\n",
       " 'run_settings_path': '/afs/inf.ed.ac.uk/user/s13/s1320903/Neuroglycerin/neukrill-net-work/run_settings/cifar10_based.json',\n",
       " 'train_split': 0.8,\n",
       " 'yaml file': 'cifar10.yaml'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now the settings can be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neukrill_net.utils.save_run_settings(run_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"augmentation_factor\":1,\r\n",
      "    \"final_shape\":[\r\n",
      "        48,\r\n",
      "        48\r\n",
      "    ],\r\n",
      "    \"model type\":\"pylearn2\",\r\n",
      "    \"preprocessing\":{\r\n",
      "        \"resize\":[\r\n",
      "            48,\r\n",
      "            48\r\n",
      "        ]\r\n",
      "    },\r\n",
      "    \"run_settings_path\":\"/afs/inf.ed.ac.uk/user/s13/s1320903/Neuroglycerin/neukrill-net-work/run_settings/cifar10_based.json\",\r\n",
      "    \"train_split\":0.8,\r\n",
      "    \"yaml file\":\"cifar10.yaml\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat run_settings/cifar10_based.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training the model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python train.py run_settings/cifar10_based.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying to run the model, it broke with an error that `partialSum` does not divide `numModules`. Turns out that `partialSum` is a parameter of a convolutional layer that affects the performance of the weight gradient computation and it has to divide the area of the output grid in this layer, which is given by `numModules`. Conveniently, the error gave the values of numModules (which are not specified in the yaml file) so we just changed partialSum in each layer to a factor of the corresponding numModules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!obj:pylearn2.train.Train {\n",
    "    dataset: &train !obj:neukrill_net.dense_dataset.DensePNGDataset {\n",
    "            settings_path: %(settings_path)s,\n",
    "            run_settings: %(run_settings_path)s,\n",
    "            training_set_mode: \"train\"\n",
    "    },\n",
    "    model: !obj:pylearn2.models.mlp.MLP {\n",
    "        batch_size: &batch_size 128,\n",
    "        layers: [\n",
    "                 !obj:pylearn2.models.maxout.MaxoutConvC01B {\n",
    "                     layer_name: 'h0',\n",
    "                     pad: 4,\n",
    "                     tied_b: 1,\n",
    "                     W_lr_scale: .05,\n",
    "                     b_lr_scale: .05,\n",
    "                     num_channels: 96,\n",
    "                     num_pieces: 2,\n",
    "                     kernel_shape: [8, 8],\n",
    "                     pool_shape: [4, 4],\n",
    "                     pool_stride: [2, 2],\n",
    "                     irange: .005,\n",
    "                     max_kernel_norm: .9,\n",
    "                     partial_sum: 49,\n",
    "                 },\n",
    "                 !obj:pylearn2.models.maxout.MaxoutConvC01B {\n",
    "                     layer_name: 'h1',\n",
    "                     pad: 3,\n",
    "                     tied_b: 1,\n",
    "                     W_lr_scale: .05,\n",
    "                     b_lr_scale: .05,\n",
    "                     num_channels: 192,\n",
    "                     num_pieces: 2,\n",
    "                     kernel_shape: [8, 8],\n",
    "                     pool_shape: [4, 4],\n",
    "                     pool_stride: [2, 2],\n",
    "                     irange: .005,\n",
    "                     max_kernel_norm: 1.9365,\n",
    "                     partial_sum: 23,\n",
    "                 },\n",
    "                 !obj:pylearn2.models.maxout.MaxoutConvC01B {\n",
    "                     pad: 3,\n",
    "                     layer_name: 'h2',\n",
    "                     tied_b: 1,\n",
    "                     W_lr_scale: .05,\n",
    "                     b_lr_scale: .05,\n",
    "                     num_channels: 192,\n",
    "                     num_pieces: 2,\n",
    "                     kernel_shape: [5, 5],\n",
    "                     pool_shape: [2, 2],\n",
    "                     pool_stride: [2, 2],\n",
    "                     irange: .005,\n",
    "                     max_kernel_norm: 1.9365,\n",
    "                 },\n",
    "                 !obj:pylearn2.models.maxout.Maxout {\n",
    "                    layer_name: 'h3',\n",
    "                    irange: .005,\n",
    "                    num_units: 500,\n",
    "                    num_pieces: 5,\n",
    "                    max_col_norm: 1.9\n",
    "                 },\n",
    "                 !obj:pylearn2.models.mlp.Softmax {\n",
    "                     max_col_norm: 1.9365,\n",
    "                     layer_name: 'y',\n",
    "                     n_classes: %(n_classes)i,\n",
    "                     irange: .005\n",
    "                 }\n",
    "                ],\n",
    "        input_space: !obj:pylearn2.space.Conv2DSpace {\n",
    "            shape: &window_shape [32, 32],\n",
    "            num_channels: 3,\n",
    "            axes: ['c', 0, 1, 'b'],\n",
    "        },\n",
    "    },\n",
    "    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
    "        learning_rate: .17,\n",
    "        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {\n",
    "            init_momentum: .5\n",
    "            },\n",
    "        train_iteration_mode: 'even_shuffled_sequential',\n",
    "        monitor_iteration_mode: 'even_sequential',\n",
    "        monitoring_dataset:\n",
    "            {\n",
    "                'test' : !obj:neukrill_net.dense_dataset.DensePNGDataset  {\n",
    "                                settings_path: %(settings_path)s,\n",
    "                                run_settings: %(run_settings_path)s,\n",
    "                                training_set_mode: \"test\"\n",
    "                },\n",
    "            },\n",
    "        cost: !obj:pylearn2.costs.mlp.dropout.Dropout {\n",
    "            input_include_probs: { 'h0' : .8 },\n",
    "            input_scales: { 'h0' : 1. }\n",
    "        },\n",
    "        termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {\n",
    "            max_epochs: 474 \n",
    "        },\n",
    "    },\n",
    "    extensions: [\n",
    "        !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {\n",
    "            start: 1,\n",
    "            saturate: 250,\n",
    "            final_momentum: .65\n",
    "        },\n",
    "        !obj:pylearn2.training_algorithms.sgd.LinearDecayOverEpoch {\n",
    "            start: 1,\n",
    "            saturate: 500,\n",
    "            decay_factor: .01\n",
    "        },\n",
    "        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {\n",
    "            channel_name: test_y_misclass,\n",
    "            save_path: '%(save_path)s'\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The results were not very good (nll = ~3) so we are not going to continue working on this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
